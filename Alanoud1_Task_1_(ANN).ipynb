{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cda1ba95",
      "metadata": {
        "id": "cda1ba95"
      },
      "source": [
        "# Exam on Artificial Neural Networks (ANN)\n",
        "\n",
        "Welcome the Artificial Neural Networks (ANN) practical exam. In this exam, you will work on a classification task to predict the outcome of incidents involving buses. You are provided with a dataset that records breakdowns and delays in bus operations. Your task is to build, train, and evaluate an ANN model.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "### **Dataset:**\n",
        "* Just run the command under the `Load Data` section to get the data downloaded and unzipped or you can access it [here](www.kaggle.com/datasets/khaledzsa/bus-breakdown-and-delays)\n",
        "\n",
        "### **Dataset Name:** Bus Breakdown and Delays\n",
        "\n",
        "### **Description:**  \n",
        "The dataset contains records of incidents involving buses that were either running late or experienced a breakdown. Your task is to predict whether the bus was delayed or had a breakdown based on the features provided.\n",
        "\n",
        "### **Features:**\n",
        "The dataset contains the following columns:\n",
        "\n",
        "- `School_Year`\n",
        "- `Busbreakdown_ID`\n",
        "- `Run_Type`\n",
        "- `Bus_No`\n",
        "- `Route_Number`\n",
        "- `Reason`\n",
        "- `Schools_Serviced`\n",
        "- `Occurred_On`\n",
        "- `Created_On`\n",
        "- `Boro`\n",
        "- `Bus_Company_Name`\n",
        "- `How_Long_Delayed`\n",
        "- `Number_Of_Students_On_The_Bus`\n",
        "- `Has_Contractor_Notified_Schools`\n",
        "- `Has_Contractor_Notified_Parents`\n",
        "- `Have_You_Alerted_OPT`\n",
        "- `Informed_On`\n",
        "- `Incident_Number`\n",
        "- `Last_Updated_On`\n",
        "- `Breakdown_or_Running_Late` (Target Column)\n",
        "- `School_Age_or_PreK`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c2b014b",
      "metadata": {
        "id": "4c2b014b"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ad02f5",
      "metadata": {
        "id": "98ad02f5"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d khaledzsa/bus-breakdown-and-delays\n",
        "#!unzip bus-breakdown-and-delays.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e39620c",
      "metadata": {
        "id": "3e39620c"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NM4pmo7PQMfA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM4pmo7PQMfA",
        "outputId": "2f0f9bea-22f5-4af3-aa12-3cbd7a78781b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "ERROR: unknown command \"unstall\" - maybe you meant \"uninstall\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip unstall pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6tknR7Qp8hVG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "6tknR7Qp8hVG",
        "outputId": "6efd77ce-e856-4e20-9f3a-0f16c0c570ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fa619f3e18b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas._libs import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPeriod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mTimedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mTimestamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_parser\u001b[0m  \u001b[0;31m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datetime\u001b[0m  \u001b[0;31m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m from pandas._libs.tslibs import (\n\u001b[1;32m     20\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32minterval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mhashtable.pyx\u001b[0m in \u001b[0;36minit pandas._libs.hashtable\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmissing.pyx\u001b[0m in \u001b[0;36minit pandas._libs.missing\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcHtZ8X5xXo6",
      "metadata": {
        "id": "fcHtZ8X5xXo6"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/Bus_Breakdown_and_Delays.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ccd4e2",
      "metadata": {
        "id": "71ccd4e2"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "This could include:\n",
        "* **Inspect the dataset**\n",
        "\n",
        "* **Dataset structure**\n",
        "\n",
        "* **Summary statistics**\n",
        "\n",
        "* **Check for missing values**\n",
        "\n",
        "* **Distribution of features**\n",
        "\n",
        "* **Categorical feature analysis**\n",
        "\n",
        "* **Correlation matrix**\n",
        "\n",
        "* **Outlier detection**\n",
        "\n",
        "And add more as needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b800b0c",
      "metadata": {
        "id": "0b800b0c"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XO6YtLX3xkyz",
      "metadata": {
        "id": "XO6YtLX3xkyz"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_kPIM7Rxnm8",
      "metadata": {
        "id": "b_kPIM7Rxnm8"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLAxfTLwxqz_",
      "metadata": {
        "id": "GLAxfTLwxqz_"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j6CJvT9Mxti1",
      "metadata": {
        "id": "j6CJvT9Mxti1"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nwUMmalBx4xg",
      "metadata": {
        "id": "nwUMmalBx4xg"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vUsxdxFbx-2G",
      "metadata": {
        "id": "vUsxdxFbx-2G"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a559e40",
      "metadata": {
        "id": "1a559e40"
      },
      "source": [
        "## Data Preprocessing\n",
        "This could include:\n",
        "\n",
        "* **Handle Missing Values**\n",
        "    * Impute missing values or drop them.\n",
        "\n",
        "* **Encode Categorical Variables**\n",
        "    * One-hot encoding\n",
        "    * Label encoding\n",
        "\n",
        "* **Scale and Normalize Data**\n",
        "    * Standardization (Z-score)\n",
        "    * Min-Max scaling\n",
        "\n",
        "* **Feature Engineering**\n",
        "    * Create new features\n",
        "    * Feature selection\n",
        "\n",
        "* **Handle Imbalanced Data**\n",
        "    * Oversampling\n",
        "    * Undersampling\n",
        "\n",
        "* **Handle Outliers**\n",
        "    * Remove outliers\n",
        "    * Transform outliers\n",
        "\n",
        "* **Remove Duplicates**\n",
        "    * Remove redundant or duplicate data\n",
        "\n",
        "\n",
        "And add more as needed!\n",
        "\n",
        "Please treat these as suggestions. Feel free to use your judgment for the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6867a7c",
      "metadata": {
        "id": "e6867a7c"
      },
      "outputs": [],
      "source": [
        "Incident_Number=df.pop('Incident_Number')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1SsmePvC0Zo-",
      "metadata": {
        "id": "1SsmePvC0Zo-"
      },
      "outputs": [],
      "source": [
        " df['How_Long_Delayed'].fillna(df['How_Long_Delayed'].mode()[0], inplace=True)\n",
        " df['Run_Type'].fillna(df['Run_Type'].mode()[0], inplace=True)\n",
        " df['Route_Number'].fillna(df['Route_Number'].mode()[0], inplace=True)\n",
        " df['Boro'].fillna(df['Boro'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5_s6NcLvPLWc",
      "metadata": {
        "id": "5_s6NcLvPLWc"
      },
      "outputs": [],
      "source": [
        "lable_encoder = LabelEncoder()\n",
        "\n",
        "df['Run_Type'] = lable_encoder.fit_transform(df['Run_Type'])\n",
        "df['Route_Number'] = lable_encoder.fit_transform(df['Route_Number'])\n",
        "df['Boro'] = lable_encoder.fit_transform(df['Boro'])\n",
        "\n",
        "y = df['Breakdown_or_Running_Late']\n",
        "X = df.drop(['Breakdown_or_Running_Late'], axis=1) # Assuming df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L8u1jCLT6hrT",
      "metadata": {
        "id": "L8u1jCLT6hrT"
      },
      "outputs": [],
      "source": [
        "lable_encoder = LabelEncoder()\n",
        "df['Run_Type'] = lable_encoder.fit_transform(df['Run_Type', 'Route_Number', 'Boro'])\n",
        "\n",
        "\n",
        "y = df['Breakdown_or_Running_Late']\n",
        "X = df_encoded.drop(['Breakdown_or_Running_Late'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hE552vZTPA3g",
      "metadata": {
        "id": "hE552vZTPA3g"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ohe = OneHotEncoder()\n",
        "#df_encoded_array = ohe.fit_transform(df[['Run_Type', 'Route_Number', 'Boro']]).toarray()\n",
        "\n",
        "#feature_names = ohe.get_feature_names_out(['Run_Type', 'Route_Number', 'Boro'])\n",
        "#df_encoded = pd.DataFrame(df_encoded_array, columns=feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc887660",
      "metadata": {
        "id": "bc887660"
      },
      "source": [
        "## Split the Dataset\n",
        "Next, split the dataset into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LGgPjIgV35uf",
      "metadata": {
        "id": "LGgPjIgV35uf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7119b7d7",
      "metadata": {
        "id": "7119b7d7"
      },
      "source": [
        "## Building the ANN Model\n",
        "In this section, define the architecture of the ANN by specifying the number of layers, neurons, and activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8532b3b9",
      "metadata": {
        "id": "8532b3b9"
      },
      "outputs": [],
      "source": [
        " model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5e52e1",
      "metadata": {
        "id": "ac5e52e1"
      },
      "source": [
        "## Compile the Model\n",
        "Compile the ANN model by defining the optimizer, loss function, and evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab363be3",
      "metadata": {
        "id": "ab363be3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a72223",
      "metadata": {
        "id": "e9a72223"
      },
      "source": [
        "## Training the Model\n",
        "Train the ANN model using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43fedab",
      "metadata": {
        "id": "e43fedab"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ce9661",
      "metadata": {
        "id": "20ce9661"
      },
      "source": [
        "## Evaluate the Model\n",
        "Evaluate the performance of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73167afc",
      "metadata": {
        "id": "73167afc"
      },
      "outputs": [],
      "source": [
        "#loss,accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss}\")\n",
        "print(f\"Test accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e9bc87",
      "metadata": {
        "id": "08e9bc87"
      },
      "source": [
        "## Make Predictions\n",
        "Use the trained model to make predictions on new or unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940fa394",
      "metadata": {
        "id": "940fa394"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94942463",
      "metadata": {
        "id": "94942463"
      },
      "source": [
        "## Model Performance Visualization\n",
        "Visualize the performance metrics such as accuracy and loss over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1955952",
      "metadata": {
        "id": "b1955952"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d32965f",
      "metadata": {
        "id": "1d32965f"
      },
      "source": [
        "## Save the Model\n",
        "Save the trained model for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5zAtpHhrCmtU",
      "metadata": {
        "id": "5zAtpHhrCmtU"
      },
      "outputs": [],
      "source": [
        "model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebe9b8d",
      "metadata": {
        "id": "9ebe9b8d"
      },
      "source": [
        "## Project Questions:\n",
        "\n",
        "1. **Data Preprocessing**: Explain why you chose your specific data preprocessing techniques (e.g., normalization, encoding). How did these techniques help prepare the data for training the model?\n",
        "2. **Model Architecture**: Describe the reasoning behind your model’s architecture (e.g., the number of layers, type of layers, number of neurons, and activation functions). Why did you believe this architecture was appropriate for the problem at hand?\n",
        "3. **Training Process**: Discuss why you chose your batch size, number of epochs, and optimizer. How did these choices affect the training process? Did you experiment with different values, and what were the outcomes?\n",
        "4. **Loss Function and Metrics**: Why did you choose the specific loss function and evaluation metrics? How do they align with the objective of the task (e.g., regression vs classification)?\n",
        "5. **Regularization Techniques**: If you used regularization techniques such as dropout or weight decay, explain why you implemented them and how they influenced the model's performance.\n",
        "6. **Model Evaluation**: Justify your approach to evaluating the model. Why did you choose the specific performance metrics, and how do they reflect the model's success in solving the task?\n",
        "7. **Model Tuning (If Done)**: Describe any tuning you performed (e.g., hyperparameter tuning) and why you felt it was necessary. How did these adjustments improve model performance?\n",
        "8. **Overfitting and Underfitting**: Analyze whether the model encountered any overfitting or underfitting during training. What strategies could you implement to mitigate these issues?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Neurons and Activation Functions\n",
        "Architecture Justification:\n",
        "\n",
        "Neurons: Using 64 neurons in the first hidden layer and 32  This helps the model capture complex patterns without overcomplicating\n",
        "\n",
        "Activation Functions:\n",
        "ReLU in hidden layers helps in mitigating the vanishing gradient problem and introduces non-linearity.\n",
        "Sigmoid in the output layer is suitable for binary classification\n",
        "\n",
        "2.  \n",
        "\n",
        "Batch Size: 32 is a good choice ,10 epochs might be short; monitor validation loss to determine if more epochs are needed.\n",
        "Optimizer (Adam): An effective choice due to its adaptive learning rates and momentum, helping in faster convergence.\n",
        "\n",
        "3.  \n",
        "\n",
        "Binary Crossentropy is appropriate for binary classification, accuracy of predicted\n",
        "Suitable for balanced datasets, indicating the proportion of correct predictions.\n",
        "\n",
        "4.  \n",
        "\n",
        "Although not mentioned, techniques like dropout and weight decay are often used to prevent overfitting. Dropout helps by randomly ignoring neurons during training\n",
        "\n",
        "5. Model Evaluation\n",
        "Evaluation Approach:\n",
        "\n",
        "Accuracy and Loss are used to measure model performance. Accuracy gives the proportion of correct predictions, while loss reflects how well the predicted values match actual outcomes.\n",
        "\n",
        "6. Model Tuning\n",
        "Adjustments:\n",
        "\n",
        "Hyperparameter Tuning: Experimenting with different parameters like learning rate, batch size, or network architecture can improve model performance. Adjustments help in finding the best configuration for the task.\n",
        "7. Overfitting and Underfitting\n",
        "\n",
        "Analysis and Strategies:\n",
        "\n",
        "Indicated by high training accuracy but low validation accuracy.  \n",
        "Underfitting: Indicated by poor performance on both\n",
        "training and validation sets."
      ],
      "metadata": {
        "id": "ccT0mV-meJWk"
      },
      "id": "ccT0mV-meJWk"
    },
    {
      "cell_type": "markdown",
      "id": "5f524a61",
      "metadata": {
        "id": "5f524a61"
      },
      "source": [
        "### Answer Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.⁠ ⁠*Data Preprocessing:*\n",
        "   - Why did you choose your specific data preprocessing techniques (e.g., normalization, encoding)? How did these techniques help prepare the data for training the model?\n",
        "\n",
        "2.⁠ ⁠*Model Architecture:*\n",
        "   - Describe the reasoning behind your model’s architecture (e.g., the number of layers, type of layers, number of neurons, and activation functions). Why did you believe this architecture was appropriate for the problem at hand?\n",
        "\n",
        "3.⁠ ⁠*Training Process:*\n",
        "   - Discuss why you chose your batch size, number of epochs, and optimizer. How did these choices affect the training process? Did you experiment with different values, and what were the outcomes?\n",
        "\n",
        "4.⁠ ⁠*Loss Function and Metrics:*\n",
        "   - Why did you choose the specific loss function and evaluation metrics? How do they align with the objective of the task (e.g., regression vs classification)?\n",
        "\n",
        "5.⁠ ⁠*Regularization Techniques:*\n",
        "   - If you used regularization techniques such as dropout or weight decay, explain why you implemented them and how they influenced the model's performance."
      ],
      "metadata": {
        "id": "8IJnMmAPeHu9"
      },
      "id": "8IJnMmAPeHu9"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}